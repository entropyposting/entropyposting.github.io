---
title: "Predictions, LLMs, Gaussian copula"
date: 2024-06-19T09:02:34-07:00
draft: False 
---
AI has reached the 1990s and discovered that Shakespeare is a lot like rap. ![Sign me up for this class](/img/shakespeare_is_rap.png "From TechCrunch")


One of my biggest gripes with the American left is its tech illiteracy. Since the political reconfigurations of the 1990s and 2000s, the left's key figures have been either unwilling or unable to make a critique of the tech that runs the modern world - either the social technologies like modern economics or the physical technologies like computing - in a way that reflects a deep understanding of either. Say what you will about the right, but they have a deep bench of figures from midwit up to intellectual who can talk about things like modern monetary theory and provide something approaching a coherent model of how the economy fits together.

There are absolutely strong critiques of modern economic theory, which coasts on unearned legitimacy due to its position as a "science" in the universities, but boomer-style grumblings about how money is made up don't win anyone over. This is doubly true of the tech-based economy and especially of its two latest bubbles, cryptocurrency and large language models. It bums me out that the two modes of critique I see most often are the two dumbest. Either it's all made up mumbo jumbo (said by some entertainer who can barely use MS Word) or "they" are introducing new technology as a way to jump-start new social relations of production and remake society. I'll concede this kind of thinking isn't just a lefty thing, but I think that applying these critiques specifically to tech is much more prevalent on the left. 

Strangely enough, though, although I disagree with their attributions, I find my own predictions on where AI is headed are more in line with the regular tech-illiterati on than with the experts trembling about imminent AGI, hypothetical superintelligent paperclip maximizers and basilisks. I have a computer job, but I didn't get hired at OpenAI, and I'm definitely not a huge-brained scientist like Yoshua Bengio or Geoffrey Hinton. So why am I willing to believe my own low-IQ intuition above theirs? The simple answer is that I think the scientists are so focused on the science that they aren't looking past their noses.

I wouldn't go so far as to say that all malice is actually stupidity in disguise; of course anyone with the goal (however hard to realize) to build Robot God and remake the world in its image sees society in a fundamentally different way than me, and probably cares so little about how this would impact the average person that they are evil by way of apathy. My argument rests on the fact that this "generative" AI wave is still just about modeling, which is my pick for the worst science ever.

The last few decades of STEM fetishism has reified modeling in our culture and convinced us that STEMlords are dispassionate, quantitative observers of the capital-T Truth, which is both totally observable and totally quantifiable. But in reality, even models made by top-tier geniuses can have fatal flaws that don't become apparent until they are applied on a big enough scale. All you need to do is look to the case of quantitative finance and the Gaussian copula. 

If you look at the breakdown of free career fair t-shirts at engineering colleges, as I did not too long ago in my last year of school, you'd know that being a quant is hot again. Quants build mathematical models that help them discover hidden value in financial markets and invent new trades to mine it out through arbitrage. These models are often statistical models that run on big computers. Like generative AI, although the techniques are different, they approximate sampling from very complex probability distributions to model the range of possible outcomes in a given scenario. Regular computers are deterministic, though, so you have to start by approximating randomness itself, then build clever algorithms on top of that to control the type and location of randomness.

The Gaussian copula is a distribution that approximates a multivariate normal (Gaussian) distribution inside of a cube, where every variable's marginal distribution lies between 0 and 1. The Gaussian copula was a mathematical breakthrough in the early 2000s that made it simple to model the correlation between different securities. The quant finance industry loved them because they allowed you to plug in historical data about a portfolio of different assets and get back a joint probability distribution to model the expected outcomes of that portfolio in the future. 

The problem with these models wasn't clear until the global financial crisis of 2008. As it turns out, the Gaussian copula's fatal flaw was its inability to accurately model the tail of a joint distribution, where the most extreme events happen. Quants at the big banks had been using Gaussian copulae to model portfolios of exotic derivative assets like mortgage-backed securities and credit-default swaps, but couldn't accurately model the risk of the entire portfolio shitting the bed.

One of my favorite books, "Diary of A Very Bad Year", is of a series of interviews with an anonymous hedge fund manager that took place from 2007-2009. The manager, referred to as "HFM", shouts this out explicitly:

> *...and we made sure it has no arms or legs or anything it could use to enslave us. But we had a loss over the course of three days that was like a ten sigma event, meaning, you know, it should never happen based on the statistical models that underlie it. Because the model doesn’t assume that everybody else is trading the same model as you are. So that’s sort of like a meta model factor. The model doesn’t know that there are other black boxes out there.*

> *n+1: What’s a ten sigma event?*

> *HFM: Meaning that it’s ten standard deviations from the mean... meaning it’s basically impossible, you know? But it’s kind of a joke, because returns are very fat tailed, so the joke that we always say is, “Oh my God, today I had a loss that’s a six sigma event! I mean, that’s the first time that’s happened in three months!” It’s like a one in ten thousand year event, and I haven’t had one in the last three months.*

These models were developed and used by the smartest guys in the room. And they failed so spectacularly that it altered the course of the global economy. Back then, as now, quant trading desks were staffed with physics and math PhDs that thought academia was too boring and wanted to make a couple houses' worth of money a year. I don't doubt that the hyper-geniuses behind generative AI models have created an amazing, mathematically exquisite piece of technology. But I think that not even the hyper-geniuses can model the real world that well. What's more, the worst failure modes and most extreme behavior of their models may not be clear until the models really need to work properly. 

So, back to malice versus stupidity. What's my actual prediction?

I think most big narratives are bad models that eliminate complexity. It seems hard for a group of people with a single vision to grab hold and pull the cart for very long. At any given moment there are five thousand different powerful individuals tied to the front, each running in the direction of their own little special interest. It just happens that power is concentrated in the hands of capital owners, so the aggregate movement of the cart is in a direction that vaguely favors capital. I'm sure the AI wave will be no different. I think the likely answer is a boring one: there'll probably be some breakthroughs that live on forever in SaaS products we all come to love, and society will be reshaped a little in a way that allows for slightly more rent extraction. 

Ironic: I'm shooting right for the center of a distribution and ignoring proposed extremely long-tail outcomes. But if I were in charge, I'd be less worried about Skynet and a lot more worried about three million truckers whose brains have been totally cooked by talk radio and are accustomed to making $300k a year. What's gonna happen when they suddenly have nothing to do? A society with a rapidly growing class of surplus people without any way to provide for themselves is not a stable equilibrium. But I don't feel confident in any specific prediction beyond that.

I think there arem any stable equilibria and many different configurations of society that roughly work out to benefit the same powerful group. We have what we have here, we have the slightly authoritarian state-backed capitalism in China, and I bet some new rough beast's idiot kids will slouch towards Ivy League schools in the 2040s. This is kind of what made me think of the name "entropyposting". One way to interpret entropy is as randomness. A desk becomes messy without energy put in to clean it, your shoes come untied. But entropy is also complexity, an order to things that's just harder to encode and compress into a big story.

